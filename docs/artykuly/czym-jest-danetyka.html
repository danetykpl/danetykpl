<!DOCTYPE html>
<html lang="pl">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="Zanurz się w świat danych, informacjii sztucznej inteligencji" />
        <meta name="author" content="danetyk.pl" />
        <title>danetyk.pl</title>

        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="../assets/favicon.ico" />

        <!-- Bootstrap icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel="stylesheet" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">

        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />

        <!-- Code Highligh -->
        <link rel="stylesheet" href="../highlight/styles/ir-black.min.css">
        <script src="../highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>

        <!-- Add Sense -->
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8413519751616760" crossorigin="anonymous"></script>
    </head>

    <body class="d-flex flex-column">
        <main class="flex-shrink-0">

            <!-- Navigation-->
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
                <div class="container px-5">
                    <a class="navbar-brand" href="../index.html"><img src="../assets/on_transparent.png" height="30" alt=""></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                            <li class="nav-item"><a class="nav-link" href="https://discord.gg/TZYfQGsn"><i class="bi-discord"></i> Discord</a></li>
                            <li class="nav-item"><a class="nav-link" href="https://www.facebook.com/danetykpl">Facebook</a></li>
                        </ul>
                    </div>
                </div>
            </nav>

            <style>
                /* CSS for the reading progress bar */
                .progress-container {
                    position: fixed;
                    margin-top: 60px;
                    top: 0;
                    left: 0;
                    width: 100%;
                    height: 4px; /* Adjust the height as needed */


                    z-index: 9999; /* Ensure it's above other elements */
                }

                .reading-progress {
                    height: 100%;
                    width: 0;
                    background-color: #ffc822; /* Color of the progress bar */
                }
            </style>

            <script>
                // JavaScript to update the reading progress bar
                window.addEventListener('scroll', () => {
                    const scrollableHeight = document.documentElement.scrollHeight - window.innerHeight;
                    const scrolled = (window.scrollY / scrollableHeight) * 100;
                    document.querySelector('.reading-progress').style.width = scrolled + '%';
                });
            </script>

            <!-- Reading progress bar container -->
            <div class="progress-container">
                <div class="reading-progress"></div>
            </div>

            <!-- Page Content-->
            <section class="py-5">
                <div class="container px-5 my-5">
                    <div class="row gx-5">
                        <div class="col-lg-3">
                            <div class="d-flex align-items-center mt-lg-5 mb-4">
                                <img class="img-fluid rounded-circle" src="../assets/danetykpl_small.png" alt="..." />
                                <div class="ms-3">
                                    <div class="fw-bold">danetyk.pl</div>
                                    <div class="text-muted">Danetyk na Dorobku</div>
                                </div>
                            </div>
                        </div>
                        <div class="col-lg-9">

                            <!-- Post content-->
                            <article>

                                <!-- Post header-->
                                <header class="mb-4">

                                    <!-- Post title-->
                                    <h1 class="fw-bolder mb-1">Czym Jest Danetyka?</h1>

                                    <!-- Post meta content-->
                                    <br>
<!--                                    <div class="text-muted fst-italic mb-2">January 1, 2023</div>-->

                                    <!-- Post categories-->
                                    <div class="badge bg-warning bg-gradient rounded-pill mb-2">Artykuły</div> <div class="badge bg-dark bg-gradient rounded-pill mb-2">Teoria</div>
                                </header>

                                <!-- Preview image figure-->
                                <figure class="mb-4"><img class="img-fluid rounded" src="../assets/artykuly/artykul_1/czym_jest_danetyka.png" alt="..." /></figure>

                                <!-- Post content-->
                                <section class="mb-5">

                                    <h3>Na początek odrobina historii</h3>

                                    <p>Jeżeli choć odrobinę interesujecie się programowaniem i tematami pokrewnymi, a ostatnich 10 lat nie spędziliście
                                    na Erasmusie wśród Amiszów, to nie ma opcji, żebyście nie spotkali się do tej pory z angielskim terminem <em>Data
                                    Science</em> (pl. <em>nauka o danych</em>, czy też <em>danetyka/danologia</em> - do wyboru do koloru...).
                                    Jeżeli jednak jakimś cudem ominęły Was ostatnie lata życia w cywilizacji, to pozwólcie, że opowiem Wam krótką
                                    historyjkę.</p>

                                    <p>Wcale nie tak dawno, bo w 1962 roku, ale również wcale nie tak blisko, bo na kampusie Uniwersytetu w Princeton,
                                    pomieszkiwał i popracowywał sobie za pieniądze amerykańskich podatników
                                    <a href="https://en.wikipedia.org/wiki/John_Tukey">John Tukey</a>. Był to jegomość o szczególnie
                                    wyróżniających się umiejętnościach we władaniu matematyką i statystyką (opracował między innymi szybką transformatę Fouriera, w skrócie FFT, a także wymyślił wykres skrzynkowy). John Tukey wyróżniał się jednak jak na swoje
                                    czasy nie tylko umiejętnościami ścisłymi, ale również humanistycznymi, bo parał się, dość biegle zresztą,
                                    jasnowidztwem. Przewidział mianowicie, że w bliskiej przyszłości komputery znajdą swoje zastosowanie
                                    w rozwiązywaniu problemów statystycznych tak, jak do tej pory służyły w rozwiązywaniu problemów matematycznych.
                                    Ostatecznie, wieszczył, nastąpi połączenie metod matematycznych, metod statystycznych oraz metod komputerowych
                                    do nowej, zunifikowanej, interdyscyplinarnej dziedziny skupiającej się na analizie danych.</p>

                                    <p>Swoje przemyślenia opisał dość szczegółowo w swoim artykule z 1962 roku zatytułowanym, notabene, <em>The Future of Data
                                    Analysis</em> <a href="#bibliografia">[1]</a>, w którym najprawdopodobniej jako pierwszy użył terminu <em>data analysis</em> (pl. <em>analiza
                                    danych</em>) w kontekście przetwarzania, analizy i modelowania danych w interdyscyplinarnym podejściu łączącym metody matematyczne,
                                    statystyczne oraz informatyczne, korzystając z dobrodziejstw komputerów.</p>

                                    <p>Co ważne, zwracał on również uwagę, że analiza danych powinna się skupiać przede wszystkim na zrozumieniu danych
                                    oraz zjawisk, które te dane opisują, a nie po prostu na dopasowywaniu istniejących modeli i teorii do danych w celu
                                    ich wyjaśnienia. Ta zmiana dotychczasowego paradygmatu otworzyła drzwi dla eksploracyjnej analizy danych (ang.
                                    <em>exploratory data analysis</em> - EDA), której głównym celem jest zrozumienie struktury danych i występujących w nich
                                    trendów oraz zależności, korzystając nie tylko z suchych liczb zwracanych przez statystyki opisowe, ale przede
                                    wszystkim korzystając z dobrodziejstw graficznej reprezentacji danych, czyli metod ich wizualizacji. Tukey zwracał
                                    również uwagę na konieczność traktowania EDA, jako procesu iteracyjnego, w ramach którego testujemy założenia
                                    szczątkowe i w zależności od potrzeb dostosowujemy pod kątem uzyskanych wyników pozostałe kroki przewidziane w
                                    ramach analizy danych. No dobrze, ale czym właściwie ta analiza danych jest?</p>

                                    <p>Cóż, lata sobie mijały, aż w 1974 roku Peter Naur opublikował książkę zatytułowaną <em>Concise Survey of Computer
                                    Methods</em> <a href="#bibliografia">[2]</a>, w której przedstawił definicję przetwarzania i analizy danych pozostającą aktualną do dnia
                                    dzisiejszego.
                                    Definicję tę można sparafrazować do następującej formy:</p>

                                    <blockquote class="blockquote">
                                        <p class="mb-0">Przetwarzanie i analiza danych, to zbiór procesów które na podstawie zbioru danych A tworzą zbiór danych B
                                        zapewniający przy pomyślnych wiatrach nowe informacje względem zbioru A. Nowy zbiór danych B powinien pozwalać
                                        na realizację bardziej efektywnych działań, niż zbiór A.</p>
                                    </blockquote>

                                    <p>Dziś należy dodać jeszcze do przetwarzania i analizy danych modelowanie danych; istniało ono oczywiście wcześniej,
                                    ale ogólnie rzecz biorąc nie w takiej formie, jak istnieje i jest rozumiane obecnie; generalizuję, ale nie chcę
                                    teraz wchodzić za bardzo w szczegóły. Niemniej, na każdym z wymienionych etapów może być konieczne cofnięcie się do
                                    etapu poprzedniego, tj. po przetworzeniu danych i rozpoczęciu pierwszej tury analiz możemy dojść do wniosku, że
                                    powinniśmy przetworzyć dane raz jeszcze, tym razem inaczej, po czym powtórzyć analizy itd. Procesy przetwarzania,
                                    analizy i modelowania danych trwają dopóty, dopóki nie uzyskamy satysfakcjonujących nas wniosków/efektów.</p>
                                    <p>Wszystko można przedstawić wizualnie w sposób następujący:</p>

                                    <figure class="mb-4"><img class="img-fluid rounded" src="../assets/artykuly/artykul_1/diagram_1.png" alt="..." /></figure>

                                    <p>No dobrze, a co wchodzi w skład każdego z wymienionych procesów oraz jakie pełnią one role? Najlepiej będzie jeśli
                                    po prostu przyjrzymy się w sposób bardziej drobiazgowy każdemu z nich. Jednak zanim to zrobimy, to szybka odpowiedź
                                    na pytanie o to, po co w ogóle to robimy. Czyli po co "danetykujemy"?</p>

                                    <h3>Dane a informacje</h3>

                                    <p>Zacznijmy od szybkiego wytłumaczenia czym są dane oraz czym są informacje. Dane, to suche fakty, których odpowiednie
                                    przetworzenie i analiza mogą zapewnić informacje, czyli użyteczne wnioski z punktu widzenia interesariusza
                                    (ang. <em>stakeholder</em>), czyli osoby, która dzięki tym wnioskom będzie mogła usprawnić wybrany aspekt działania
                                    np. swojej firmy. Dla przykładu, danymi mogą być wiek, wykształcenie oraz dochód klientów banku, natomiast
                                    informacją może być częstość zaciągania kredytów gotówkowych w zależności od wymienionych "parametrów" klientów.</p>

                                    <blockquote class="blockquote">
                                        <p class="mb-0">W skrócie, dane, to suche fakty, natomiast informacje, to użyteczne wnioski.</p>
                                    </blockquote>

                                    <p>Oczywiście, nic nie stoi na przeszkodzie, aby informacje stały się danymi, na podstawie których zostaną
                                    wyciągnięte inne wnioski, czyli powstaną nowe informacje i tak w koło Macieju.</p>

                                    <h3>A dlaczego? A komu to potrzebne?</h3>

                                    <p>No dobrze, danetykujemy, żeby dane przekuwać w informacje. A w jakich branżach i w jakich celach? Dla przykładu:</p>

                                    <ul>
                                        <li>Energetyka (np. analiza przepustowości sieci przesyłowych dla wielu źródeł energii różnego typu dla różnych lokalizacji i różnych pór dnia i roku)</li>
                                        <li>Transport (np. analiza zagęszczenia ruchu drogowego na wybranych odcinkach w zależności od pory dnia oraz pracy sygnalizacji świetlnej)</li>
                                        <li>Logistyka (np. analiza wolumenu transportowego, jego szybkości i kosztów w zależności od rodzaju transportu)</li>
                                        <li>Medycyna (np. analiza danych pacjentów pod kątem optymalizacji zespołu wybranego szpitala pod kątem dobrania odpowiedniej liczby lekarzy danych specjalności)</li>
                                        <li>Badania kliniczne (np. analiza skuteczności nowych leków w zależności od charakterystyki pacjentów)</li>
                                        <li>Diagnostyka (np. opracowywanie zautomatyzowanych rozwiązań i metod diagnostycznych)</li>
                                        <li>Farmacja (np. przyspieszenie procesu wynajdywania nowych związków o potencjale leczniczym)</li>
                                        <li>Produkcja (np. usprawnianie i optymalizacja procesów i linii produkcyjnych)</li>
                                        <li>Motoryzacja (np. rozwój rozwiązań autonomicznych zwiększających bezpieczeństwo na drodze)</li>
                                        <li>E-Commerce (np. analiza danych klientów pod kątem usprawnienia personalizacji reklam, ofert i rekomendacji produktów)</li>
                                        <li>Cyberbezpieczeństwo (np. usprawnienie procesów wykrywania oszustw i exploitów)</li>
                                        <li>Finanse (np. analiza rynków, szacowanie ryzyka inwestycji, czy też choćby personalizacja ofert produktów finansowych dla klientów banków w zależności od ich profilu)</li>
                                        <li>I wiele innych... Gdzie dla przykładu możemy napomknąć o "sztucznych inteligencjach" wykorzystywanych obecnie niemal do wszystkiego, przez wszystkich i wszędzie (polecam przy okazji film "Wszystko wszędzie naraz" - perełka)</li>
                                    </ul>

                                    <p>Jak więc widzimy danetykować możemy praktycznie w dowolnej branży, gdzie na podstawie danych tworzone są informacje.
                                    A z uwagi na fakt, że w dzisiejszych czasach robi się to niemal wszędzie, to i niemal wszędzie każdy znajdzie
                                    coś dla siebie. Innymi słowy, pracy nie zabraknie (no chyba, że wygryzie nas AI hue hue hue).</p>

                                    <blockquote class="blockquote">
                                        <p class="mb-0">Co ciekawe, ilość przetwarzanych danych na całym świecie w roku 2010 wynosiła 2 zetabajty. W roku 2022 było to już 97
                                        zetabajtów. Natomiast przewiduje się (liberalnie), że w roku 2025 będzie to już 181 zetabajtów. Zwracam tutaj uwagę
                                        na to, że jeden zetabajt, to jeden bilion, czyli tysiąc miliardów gigabajtów. Także ten, no, sporo.</p>
                                    </blockquote>

                                    <h3>Czym jest przetwarzanie danych?</h3>

                                    <p>Rzadko kiedy dane (czyt. nigdy), na których mamy zrealizować jeszcze bliżej nieokreślone analizy, do tych analiz się
                                    w ogóle na początku nadają. Najczęściej bowiem jest tak, że najpierw musimy je przejrzeć, zrozumieć ich strukturę,
                                    scharakteryzować ich typy oraz zrozumieć ich znaczenie. Gdy już rozumiemy na co właściwie patrzymy na ekranach
                                    naszych komputerów, to należy to, na co patrzymy, doprowadzać do relatywnego stanu użyteczności. To znaczy,
                                    sprawdzamy dane pod kątem występowania w nich duplikatów, identyfikujemy i zaradzamy brakom w danych, a także dane
                                    agregujemy. Najogólniej rzecz biorąc ugniatamy dane do momentu, aż przyjmą formę, która da się w sposób sensowny
                                    przeanalizować. A co to znaczy? Cóż, ciężko jest to opisać bez konkretnych przykładów, co postaram się zapewnić w
                                    następnych materiałach. Jednak najczęściej realizujemy następujące kroki w ramach przetwarzania danych:</p>

                                    <ol>
                                        <li>Sprawdzić/określić typy danych w poszczególnych kolumnach i zweryfikować zgodność wprowadzonych wartości z ich domniemanymi typami.</li>
                                        <li>Poprawić błędy/literówki w wybranych wartościach, jeżeli takowe występują.</li>
                                        <li>Poradzić sobie z brakującymi wartościami.</li>
                                        <li>Poradzić sobie z duplikatami wartości.</li>
                                        <li>Jeżeli zajdzie taka potrzeba, to przekonwertować wybrane typy danych na inne typy.</li>
                                        <li>Przeprowadzić normalizację wybranych zmiennych, czyli sprowadzić je do wspólnej, porównywalnej skali.</li>
                                        <li>Przeprowadzić kodowanie zmiennych kategorialnych, czyli np. dla zmiennej <code>płeć</code> zmienić wartości <code>kobieta</code> i <code>mężczyzna</code> na <code>0</code> i <code>1</code>.</li>
                                        <li>Przefiltrować, przekształcić, zredukować dane do pożądanego zakresu.</li>
                                        <li>Powtarzamy maglowanie do momentu, aż dane przyjmą sensowną, dającą się analizować formę.</li>
                                    </ol>

                                    <h3>Czym jest analizowanie danych?</h3>

                                    <p>Najogólniej rzecz biorąc celem analizowania danych jest zapewnienie na ich podstawie użytecznych informacji.
                                    Najczęściej więc zaczynamy od wspomnianej już wcześniej eksploracyjnej analizy danych (EDA), w ramach której
                                    wykorzystujemy statystyki opisowe oraz różnego rodzaju wizualizacje (np. histogramy, wykresy skrzynkowe, wykresy
                                    rozrzutu). Celem EDA jest charakteryzacja rozkładów danych, jak również identyfikacja potencjalnych trendów i
                                    zależności. Jeżeli znajdziemy w danych coś ciekawego, to następnym krokiem jest przeprowadzenie wnioskowania
                                    statystycznego, które pozwala zwalidować nasze założenia względem danych i odpowiedzieć na posiadane pytania
                                    badawcze. W ramach EDA oraz wnioskowania statystycznego najczęściej realizujemy następujące kroki:</p>

                                    <ol>
                                        <li>Analiza miar częstości (liczebność, częstość względna, procent), tendencji centralnej (średnia, mediana, moda),
                                           dyspersji (wariancja, odchylenie standardowe) oraz asymetrii (skośność, kurtoza).</li>
                                        <li>Wizualizacja danych (np. histogram, wykres skrzynkowy / rozrzutu).</li>
                                        <li>Wnioskowanie statystyczne (np. chi kwadrat, korelacja, test t, analiza wariancji).</li>
                                        <li>W zależności od potrzeb powtórzenie EDA lub nawet powtórzenie procesu przetwarzania danych przed kolejną iteracją
                                           EDA.</li>
                                    </ol>

                                    <h3>Czym jest modelowanie danych?</h3>

                                    <p>Gdy już poznaliśmy w sposób wystarczający dane dzięki ich przetwarzaniu i analizie, to możemy pójść o krok dalej i
                                    spróbować stworzyć model te dane opisujący. Po co? Przychodzą mi teraz do głowy dwa główne powody. Po pierwsze, po
                                    to, aby móc dokonywać predykcji na temat danych nieistniejących na podstawie danych istniejących - czyli tak zwane
                                    modelowanie predykcyjne (ang. <em>predictive modelling</em>). Po drugie, aby uzyskać jeszcze głębszy wgląd w dane
                                    i dostrzec istniejące w nich wzorce i zależności na poziomie, jakiego nie mogliśmy osiągnąć z wykorzystaniem EDA
                                    oraz wnioskowania statystycznego - czyli tak zwane modelowanie opisowe (ang. <em>descriptive modelling</em>).</p>

                                    <blockquote class="blockquote">
                                        <p class="mb-0">Możemy więc wyróżnić modelowanie predykcyjne oraz modelowanie opisowe.</p>
                                    </blockquote>

                                    <p>Wśród najpopularniejszych technik wykorzystywanych w modelowaniu danych możemy wymienić:</p>

                                    <ol>
                                        <li>Regresje (ang. <em>regression</em>)</li>
                                        <li>Drzewa decyzyjne (ang. <em>decision trees</em>)</li>
                                        <li>Losowy las decyzyjny (ang. <em>random forest</em>)</li>
                                        <li>Sieci neuronowe (ang. <em>neural networks</em>)</li>
                                        <li>Głębokie sieci neuronowe (ang. <em>deep neural networks</em>)</li>
                                    </ol>

                                    <hr />

                                    <p><strong>Autopromocja 🌱</strong></p>

                                    <p>Zachęcamy do zainteresowania się naszymi szkoleniami:</p>

                                    <ul>
                                        <li><a href="https://strefakursow.pl/kursy/programowanie/kurs_programowanie_w_python_dla_sredniozaawansowanych.html?ref=111440">Programowanie w Python dla średniozaawansowanych</a></li>
                                        <li><a href="https://strefakursow.pl/kursy/programowanie/fundamenty_przetwarzania_i_analizy_danych_w_sql.html?ref=111440">Fundamenty przetwarzania i analizy danych w SQL</a></li>
                                        <li><a href="https://strefakursow.pl/kursy/programowanie/fundamenty_programowania_w_python.html?ref=111440">Fundamenty programowania w Python</a></li>
                                        <li><a href="https://strefakursow.pl/kursy/programowanie/fundamenty_jezyka_java.html?ref=111440">Fundamenty języka Java</a></li>
                                    </ul>

                                    <hr />

                                    <h3>Danetyka kilka lat temu i dzisiaj</h3>

                                    <p>Jeszcze kilka lat temu mało kto mówił w Polsce o <em>data science</em>, o czym niech świadczą statystyki wyszukiwania tej
                                    frazy w wyszukiwarce Google dla naszego kraju. </p>

                                    <figure class="mb-4"><img class="img-fluid rounded" src="../assets/artykuly/artykul_1/google_trends_1.png" alt="..." /></figure>
                                    <figure class="mb-4"><img class="img-fluid rounded" src="../assets/artykuly/artykul_1/google_trends_2.png" alt="..." /></figure>

                                    <p>Wzrost zaczął się dopiero w okolicach roku 2016, co niekoniecznie dobrze świadczy o naszym pięknym kraju (tutaj bez
                                    sarkazmu - Polska jest piękna). Pozwólcie bowiem, że przedstawię Wam krótką listę wydarzeń ze świata analizy danych
                                    tylko z ostatnich kilkunastu lat.</p>

                                    <ul>
                                        <li>Rok 2002 - Powołanie do życia pierwszego czasopisma naukowego dedykowanego <em>data science</em> przez <em>Committee on Data
                                          Science and Technology</em> o nazwie <a href="https://datascience.codata.org/"><em>Data Science Journal</em></a>.</li>
                                        <li>Rok 2008 - Termin <em>data scientist</em> staje się światowym <em>viralem</em> (bynajmniej nie u nas).</li>
                                        <li>Wielkie korporacje, jak Google, czy Facebook zaczynają opierać swoje istnienie na pośredniczeniu między reklamodawcami a swoimi użytkownikami. Przetwarzanie wielkich wolumenów danych, ich analiza i budowanie modeli predykcyjnych staje się nieodłączną częścią działalności korporacji.</li>
                                        <li>Rok 2010 - Powstaje platforma <a href="https://www.kaggle.com/">Kaggle</a>, zrzeszająca osoby zajmujące się <em>data science</em>. </li>
                                        <li>Rozkręca się <em>hype</em> na sztuczną inteligencję (ang. <em>artificial intelligence</em> - AI). </li>
                                        <li>Rok 2011 - Liczba ofert pracy dla danetyków skacze o 15 000%.</li>
                                        <li>Sztuczna inteligencja stworzona przez IBM wygrywa program <em>Jeopardy!</em></li>
                                        <li>Rok 2012 - <em>Data scientist</em> zostaje okrzyknięte najseksowniejszym stanowiskiem pracy na świecie przez Harvard (cokolwiek to znaczy; ciekaw jestem metodologii stojącej za dojściem do tego wniosku) <a href="#bibliografia">[3]</a>. </li>
                                        <li>Rok 2013 - Według firmy IBM 90% wszystkich danych na świecie zostało wytworzonych w ciągu ostatnich dwóch lat.</li>
                                        <li>Rok 2014 - <em>Data scientist</em> zostaje okrzyknięte najseksowniejszym stanowiskiem pracy na świecie przez magazyn
                                          <em>Forbes</em> <a href="#bibliografia">[4]</a>. </li>
                                        <li>Rok 2015 - Dzięki wykorzystaniu uczenia głębokiego (ang. <em>deep learning</em> - jednej z form AI) efektywność systemu
                                          rozpoznawania mowy firmy Google skacze o 49%. Google zwiększyło również wykorzystanie uczenia maszynowego z dotychczasowego sporadycznego, do ponad 2 700 wewnętrznych projektów.</li>
                                        <li>Rok 2016 - Sztuczna inteligencja o nazwie <em>AlphaGo</em> stworzona przez firmę Deep Mind należącą do firmy Google pokonuje mistrza świata w grze <em>Go</em>.</li>
                                        <li>I w końcu zaczyna się zainteresowanie tematem <em>data science</em> w Polsce na poważnie.</li>
                                        <li>Rok 2021 - Sztuczna inteligencja o nazwie <em>AlphaFold</em> przewiduje struktury białek nieporównywalnie efektywniej niż
                                          dotychczasowe metody komputerowe nadzorowane przez człowieka. Z tego co się orientuję, to na chwilę obecną
                                          przewidziała struktury już praktycznie wszystkich białek znanych nauce. Warto napomknąć, że jeszcze do niedawna
                                          niczym niezwykłym było robienie doktoratu w ramach którego odkrywano i badano strukturę JEDNEGO białka. Także ten...</li>
                                        <li>Rok 2022 - Jason Allen wykorzystując obrazy wygenerowane przez sztuczną inteligencję <em>Midjourney</em>, wygrywa konkurs
                                          artystyczny, pokonując artystów tworzących swoje prace ręcznie <a href="#bibliografia">[5]</a>.</li>
                                        <li>Przełom roku 2022 i 2023 - Pojawia się <em>ChatGPT</em> firmy/fundacji OpenAI oparty o ich model GPT-3 (teraz już GPT-4, a
                                          niedługo GPT-5), przez co wielu ludzi wykonujących wiele różnych zawodów zaczyna... odrobinę się martwić.</li>
                                    </ul>

                                    <p>Oczywiście lista istotnych z punktu widzenia branży wydarzeń w ostatnich 20 latach jest o wiele obszerniejsza, ale
                                    chciałem jedynie wywrzeć na Was pewne określone wrażenie. A mianowicie, świat danych i ich przetwarzania galopuje
                                    jak oszalały i w gruncie rzeczy nie wiadomo, co przyniesie jutrzejszy dzień. Co prawda co bardziej pesymistyczne głosy
                                    mówią, że danetyka popełnia rozciągnięte w czasie samobójstwo z uwagi na rozwój AI, która może ostatecznie sprawić,
                                    że dane "będą analizowały się same". Uważam jednak, że nawet jeżeli AI trafi pod danetyczne strzechy na dobre, to
                                    będzie miejsce dla ludzi, którzy potrafią z niej korzystać, rozumieją jej działanie oraz to, co "wypluwa" w wyniku
                                    swojego działania. Innymi słowy będą potrzebni ludzie, którzy będą niejako pośredniczyć między światem zewnętrznym a technologią, czyli w gruncie rzeczy między technologią a innymi ludźmi.</p>

                                    <h3>No to czym jest ta danetyka?</h3>

                                    <p>Uf, no właśnie - to czym jest ta danetyka? Na pewno jedną z najszybciej i najbardziej dynamicznie rozwijających się
                                    dziedzin. W niespełna 50 lat zdążyła powstać, rozwinąć się i zagrozić stabilności finansowej jej twórców i wyznawców przez swój bezprecedensowy rozwój.
                                    Oczywiście najprawdopodobniej przesadzam i pomimo trudnego do przewidzenia rozwoju AI miejsce dla człowieka gdzieś w tym wszystkim
                                    na pewno się znajdzie, jak zawsze. Niemniej, kontynuując odpowiedź na główne pytanie postawione w tym artykule, danetyka, to tłumaczenie danych na informacje, to
                                    optymalizowanie procesów, zwiększanie ich efektywności, a co za tym idzie zmniejszanie kosztów finansowych i
                                    energetycznych, niezależnie od tego, kto lub co będzie odpowiedzialny/e za realizowanie tych procesów.
                                    Wiadomo, że przyszłość należy do danych. Przyszłość należy do danetyki. (i AI hue hue hue)</p>

                                    <h3 id="bibliografia">Bibliografia</h3>

                                    <ol>
                                        <li>Tukey JW (1962) The Future of Data Analysis. <a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full">Źródło</a>.</li>
                                        <li>Naur P (1974) Concise Survey of Computer Methods.</li>
                                        <li>Davenport TH &amp; Patil DJ (2012) "Data Scientist: The Sexiest Job of the 21st Century". Harvard Business Review. <a href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century">Źródło</a>. </li>
                                        <li>Magyar J (2014) "Data Scientist: Sexiest Job Of The Century?". Forbes. <a href="https://www.forbes.com/sites/sap/2014/01/21/data-scientist-sexiest-job-of-the-century/?sh=1edf3248674b">Źródło</a>.</li>
                                        <li>Harwell D (2022) "He used AI to win a fine-arts competition. Was it cheating?" Washington Post. <a href="https://www.washingtonpost.com/technology/2022/09/02/midjourney-artificial-intelligence-state-fair-colorado/">Źródło</a>.</li>
                                    </ol>
                                </section>
                            </article>
                        </div>
                    </div>
                </div>
            </section>
        </main>



        <!-- Footer-->
        <footer class="bg-dark py-4 mt-auto">
            <div class="container px-5">
                <div class="row align-items-center justify-content-between flex-column flex-sm-row">
                    <div class="col-auto"><div class="small m-0 text-white">danetyk.pl  &copy; 2023</div></div>
                    <div class="col-auto">
                        <div class="small m-0 text-white">kontakt@danetyk.pl</div>
                    </div>
                </div>
            </div>
        </footer>

        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>

        <!-- Core theme JS-->
        <script src="../js/scripts.js"></script>
    </body>
</html>
